# HPC Workflow Template 

## Overview
This repository contains a set of templates and scripts tailored for high-performance computing (HPC) in neuroscience research. It is designed to facilitate the setup and execution of computational models and data analysis pipelines on HPC systems.

## Repository Structure

```plaintext
hpc-template/
├── 📁 hpc-outputs/               # Output files from HPC computations
├── 📁 archive/                   # Archived files and results
├── 📁 figures/                   # Directory for storing figures generated by scripts
├── 📁 data/                      # Data used by HPC tasks
├── 📁 scripts/                   # Essential scripts for running analyses
│   ├── 📁 process/               # Main directory for process flow
│       ├── main_function_v3.m                 # Main analysis pipeline function
│       ├── analysis_pipeline_sub_function_1.m # Sub-function for main function
│       └── analysis_pipeline_sub_function_2.m # Sub-function for main function
│   ├── 📁 objects/               # Storage for objects used in scripts
│   ├── 📁 dependencies/          # External dependencies and libraries
│   ├── 📁 sandbox/               # Experimental scripts and trial area
│   ├── 📁 archive/               # Storage for archived scripts and data
│   └── 📁 plot-functions/        # Functions for plotting and visualizing data
├── pull.sh                       # Script to pull datasets from cluster
├── sync.sh                       # Script to synchronize data from local to cluster
├── dsq-caller.sh                 # Dispatcher for distributed jobs
├── write-jobs.sh                 # Job writing utility
└── main_caller_function.sh       # Main function to call analysis scripts in process/
```

## Installation

Clone the repository to your local machine using the following command:

```bash
git clone https://github.com/danielcalbick/hpc-template.git
```

## Usage

Ensure you have the necessary permissions to execute the scripts, use this terminal command if needed:
```bash
chmod +x *.sh
```

To get started, navigate to the hpc template directory and edit the sync.sh script to reflect your cluster ip and username. Make sure that you direct the sync.sh script to the project directory on the cluster that you want to sync with the current directory of the sync.sh scipt that you want to mirror/connect. Then, run the script to synchronize your data to the cluster:

```bash
./sync.sh
```

Next you can run the write-jobs.sh script to write the job file that will be run on the cluster. This script will write a line in a jobs.txt file for each run you want to parallelize (eg one per subject in an FMRI analysis). 

```bash
./write-jobs.sh arg1 arg2 arg3 
```

Once the job script is created we can invoke the dSQ (dead simple queue) script to run the jobs on the cluster. This script will read the jobs.txt file and submit each job to the cluster. 

```bash
./main_caller_function.sh job-file.txt arg1 arg2
```

Included is a workflow template that can be used to run a simple analysis pipeline. The main_function_v3.m script is the main analysis pipeline function that calls sub-functions to perform the analysis. The analysis_pipeline_sub_function_1.m and analysis_pipeline_sub_function_2.m scripts are examples of sub-functions that can be called by the main function. 

Once analyses are done running, you can pull the data back to your local machine using the pull.sh script, from the hpc-template directory on your local machine:

```bash
./pull.sh
```


## Contributing

Contributions to this project are welcome. Please fork the repository and submit a pull request with your additions.

## Authors

* **Daniel Calbick**  - [danielcalbick](https://github.com/danielcalbick)

